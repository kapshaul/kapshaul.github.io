<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Natural Language Processing (NLP) - Finite State Machine with RNN | Yong-Hwan Lee</title>
<meta name="keywords" content="NLP, LSTM, RNN, Sequence Modeling, Machine Learning">
<meta name="description" content="This study was carried out as a project at Oregon State University.">
<meta name="author" content="Yong-Hwan Lee">
<link rel="canonical" href="http://localhost:1313/studies/finite-state-machine/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d6cf4a8fa527330d9574f36d8d000fdaf90ca838ff09ab72fc27d3cb7ca1ddc5.css" integrity="sha256-1s9Kj6UnMw2VdPNtjQAP2vkMqDj/Caty/CfTy3yh3cU=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/studies/finite-state-machine/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Natural Language Processing (NLP) - Finite State Machine with RNN" />
<meta property="og:description" content="This study was carried out as a project at Oregon State University." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/studies/finite-state-machine/" /><meta property="article:section" content="studies" />
<meta property="article:published_time" content="2024-05-10T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-05-10T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Natural Language Processing (NLP) - Finite State Machine with RNN"/>
<meta name="twitter:description" content="This study was carried out as a project at Oregon State University."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Studies",
      "item": "http://localhost:1313/studies/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Natural Language Processing (NLP) - Finite State Machine with RNN",
      "item": "http://localhost:1313/studies/finite-state-machine/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Natural Language Processing (NLP) - Finite State Machine with RNN",
  "name": "Natural Language Processing (NLP) - Finite State Machine with RNN",
  "description": "This study was carried out as a project at Oregon State University.",
  "keywords": [
    "NLP", "LSTM", "RNN", "Sequence Modeling", "Machine Learning"
  ],
  "articleBody": "Overview This study demonstrates the power of recurrent neural networks (RNNs), particularly long short-term memory (LSTM) models, across a range of natural language processing tasks. It begins with a manually engineered LSTM for binary parity classification and progresses to training LSTM networks for generalization, embedded Reber grammar recognition, and part-of-speech tagging using a BiLSTM.\nInstallation Clone the repository and install dependencies:\ngit clone https://github.com/kapshaul/NLP-finite.state.machine.RNN cd NLP-finite.state.machine.RNN pip install torch torchtext matplotlib portalocker Implementation To test manual LSTM configuration for binary parity, run:\npython univariate_tester.py To train and evaluate LSTM models on parity tasks, run:\npython driver_parity.py To implement and train BiLSTM for POS tagging, run:\npython driver_udpos.py Demystifying Recurrent Neural Networks Hand Designing an LSTM for Parity In this section, we manually explore the capability of LSTM networks to handle sequential tasks, specifically determining the parity of binary strings (i.e., whether the number of ones is even or odd). A simple binary string parity classification can be represented by recursive XOR operations, an ideal use-case for an LSTM’s recurrent structure.\nUnivariate LSTM Setup We can consider a LSTM where inputs, outputs, and weights are scalars, defined by:\n$i_t = \\sigma(w_{ix}x_t + w_{ih}h_{t-1} + b_i)$\n$f_t = \\sigma(w_{fx}x_t + w_{fh}h_{t-1} + b_f)$\n$o_t = \\sigma(w_{ox}x_t + w_{oh}h_{t-1} + b_o)$\n$g_t = \\tanh(w_{gx}x_t + w_{gh}h_{t-1} + b_g)$\n$c_t = f_t c_{t-1} + i_t g_t$\n$h_t = o_t \\tanh(c_t)$\nManual Parameter Setting for XOR We can find weights and biases to perform parity classification manually. The goal was to have the final hidden state (h_t) ≥ 0.5 for odd parity and \u003c 0.5 for even parity. The selected weights and biases:\nInput gate: wix = 10, wih = 10, bi = -5 Forget gate: wfx = 0, wfh = 0, bf = -10 Output gate: wox = -10, woh = -10, bo = 15 Gate g: wgx = 0, wgh = 0, bg = 10 With these parameters:\ni_t acts as an OR gate. o_t acts as a NAND gate. c_t effectively behaves as an AND gate. This demonstrates that even a minimal LSTM can solve the parity problem through careful manual configuration.\nUnderstanding We have demonstrated that a single-dimensional LSTM can theoretically compute parity for binary sequences of arbitrary length, setting the foundation to later explore learning these parameters automatically.\nLearning Finite State Machines with LSTM This section explores the capacity of LSTMs to model deterministic finite state machines (FSMs), including both synthetic binary classification and structured language sequences.\nParity Task: Generalization to Longer Sequences The LSTM is trained on binary sequences of varying lengths to predict their parity (even or odd number of 1s). The dataset is generated with all binary combinations up to a max_length, and the labels are calculated as sum(seq) % 2.\nModel Summary class ParityLSTM(nn.Module): def __init__(self, hidden_dim=64): super().__init__() self.rnn = nn.LSTM(input_size=1, hidden_size=hidden_dim, batch_first=True) self.fc = nn.Linear(hidden_dim, 2) def forward(self, x, lengths): x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False) _, (h_n, _) = self.rnn(x) output = self.fc(h_n[-1]) return output Input is padded to batch format, and packed before passing to the LSTM. Final hidden state is used for classification via a fully connected output layer. Understanding (a) Hidden size = 1\r(b) Hidden size = 16\r(c) Hidden size = 256\rFigure 1: LSTM Parity Detection Accuracy with Varying Hidden Sizes\nLSTM with hidden size of 1 can learn the task to 100% accuracy, validating the theoretical result. Larger hidden sizes speed up convergence but may overfit on shorter training sequences. Generalization to sequences up to length 256 is tested, and performance drops slightly unless tuned properly. Embedded Reber Grammar (ERG): Recognizing Structured Sequences The LSTM is further challenged with a more complex task: classifying whether a string was generated by a structured Embedded Reber Grammar (ERG). To evaluate how recurrent models handle structured, long-range dependencies, we use the Embedded Reber Grammar (ERG) task. This synthetic task challenges a model to classify whether a given sequence follows the strict rules of ERG generation.\nWhat is the Embedded Reber Grammar? The Embedded Reber Grammar is a state machine used to generate sequences of characters following recursive, nested patterns. It contains two identical sub-networks (Reber grammars) that can repeat multiple times before the sequence ends.\nA valid ERG string example:\nBTBTXSEBTXSEBPVVEBTXXVVETE\nThis decomposes into:\nBT | BTXSE | BTXSE | BPVVE | BTXXVVE | TE\nThe task is to classify whether a given sequence is valid (follows ERG rules) or invalid (e.g., due to character-level perturbations).\nFigure 2: ERG generation diagram\nModels Compared Two models are evaluated on this task:\nModel Train Accuracy Validation Generalization RNN High Poor (overfits) LSTM High Strong generalization Figure 3: RNN vs LSTM\nRNN: Struggles with long-term dependencies, fails to generalize despite fitting the training set. LSTM: Learns the underlying recursive structure and performs well on unseen examples. Why LSTM Outperforms RNN? LSTM’s design includes key architectural features:\nInput, forget, and output gates allow selective memory retention. Cell state enables long-distance signal propagation without degradation. Effective for recursion and repeated structures, unlike RNNs, which suffer from vanishing gradients. As a result, LSTMs can maintain context across complex, nested subsequences — which is essential for modeling grammars like ERG.\nPart-of-Speech Tagging with BiLSTM This task applies BiLSTM models to a real-world NLP application — tagging each word in an English sentence with its corresponding part-of-speech (POS) using the UDPOS dataset.\nDataset Overview Comes with train, valid, and test splits. Includes a mix of topics (e.g., family, employment, science). POS distribution is imbalanced, so majority label baseline is used as a sanity check. Preprocessing Custom pad_collate() is used to batch variable-length sequences. Lemmatization is not applied, but could help reduce sparsity. Words are converted to token IDs via a vocabulary object or torchtext pipeline. Figure 4: POS Histogram\nBiLSTM Model Architecture class BILSTM_POS(nn.Module): def __init__(self, vocab_size, tag_size, embedding_dim=128, hidden_dim=256): super().__init__() self.embedding = nn.Embedding(vocab_size, embedding_dim) self.bilstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True) self.dropout = nn.Dropout(0.5) self.fc = nn.Linear(hidden_dim * 2, tag_size) def forward(self, x, lengths): x = self.embedding(x) x = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False) o, _ = self.bilstm(x) o, _ = pad_packed_sequence(o, batch_first=True) o = self.dropout(o) o = self.fc(o) return torch.log_softmax(o, dim=-1) Embedding layer → BiLSTM → dropout → linear output → log-softmax over tags Bidirectional structure ensures each word is contextualized with both left and right neighbors. Training Observations Training accuracy improves steadily and outpaces validation loss after ~30 epochs. Likely due to over-representation of common tokens like UNK, which default to the NOUN tag early in training. Dropout regularization helps mitigate overfitting. Figure 5: Train and Validation Loss\nLoss Trend Epoch 40/40 Train Loss: 0.0227 Valid Loss: 0.2679 Test Accuracy: 86.23% POS Tagging Inference Examples Example 1:\nThe old man the boat.\nDET ADJ NOUN DET NOUN PUNCT\nExample 2:\nThe complex houses married and single soldiers and their families.\nDET ADJ NOUN VERB CCONJ ADJ NOUN CCONJ PRON NOUN PUNCT\nExample 3:\nThe man who hunts ducks out on weekends.\nDET NOUN PRON PROPN VERB ADV ADP NOUN PUNCT\n",
  "wordCount" : "1153",
  "inLanguage": "en",
  "datePublished": "2024-05-10T00:00:00Z",
  "dateModified": "2024-05-10T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Yong-Hwan Lee"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/studies/finite-state-machine/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Yong-Hwan Lee",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Yong-Hwan Lee">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Yong-Hwan Lee</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/studies/" title="Studies">
                    <span>Studies</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Natural Language Processing (NLP) - Finite State Machine with RNN
    </h1>
    <div class="post-meta"><span title='2024-05-10 00:00:00 +0000 UTC'>May 2024</span>&nbsp;&middot;&nbsp;Yong-Hwan Lee&nbsp;&middot;&nbsp;<a href="https://github.com/kapshaul/NLP-finite.state.machine.RNN" rel="noopener noreferrer" target="_blank">GitHub</a>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#installation">Installation</a></li>
    <li><a href="#implementation">Implementation</a></li>
    <li><a href="#demystifying-recurrent-neural-networks">Demystifying Recurrent Neural Networks</a></li>
    <li><a href="#learning-finite-state-machines-with-lstm">Learning Finite State Machines with LSTM</a></li>
    <li><a href="#part-of-speech-tagging-with-bilstm">Part-of-Speech Tagging with BiLSTM</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>This study demonstrates the power of recurrent neural networks (RNNs), particularly long short-term memory (LSTM) models, across a range of natural language processing tasks. It begins with a manually engineered LSTM for binary parity classification and progresses to training LSTM networks for generalization, embedded Reber grammar recognition, and part-of-speech tagging using a BiLSTM.</p>
<hr>
<h2 id="installation">Installation<a hidden class="anchor" aria-hidden="true" href="#installation">#</a></h2>
<p>Clone the repository and install dependencies:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>git clone https://github.com/kapshaul/NLP-finite.state.machine.RNN
</span></span><span style="display:flex;"><span><span style="color:#0aa">cd</span> NLP-finite.state.machine.RNN
</span></span><span style="display:flex;"><span>pip install torch torchtext matplotlib portalocker
</span></span></code></pre></div><hr>
<h2 id="implementation">Implementation<a hidden class="anchor" aria-hidden="true" href="#implementation">#</a></h2>
<ol>
<li>
<p>To test <strong>manual LSTM configuration for binary parity</strong>, run:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python univariate_tester.py
</span></span></code></pre></div></li>
<li>
<p>To train and evaluate <strong>LSTM models on parity tasks</strong>, run:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python driver_parity.py
</span></span></code></pre></div></li>
<li>
<p>To implement and train <strong>BiLSTM for POS tagging</strong>, run:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python driver_udpos.py
</span></span></code></pre></div></li>
</ol>
<hr>
<h2 id="demystifying-recurrent-neural-networks">Demystifying Recurrent Neural Networks<a hidden class="anchor" aria-hidden="true" href="#demystifying-recurrent-neural-networks">#</a></h2>
<h4 id="hand-designing-an-lstm-for-parity">Hand Designing an LSTM for Parity<a hidden class="anchor" aria-hidden="true" href="#hand-designing-an-lstm-for-parity">#</a></h4>
<p>In this section, we manually explore the capability of LSTM networks to handle sequential tasks, specifically determining the parity of binary strings (i.e., whether the number of ones is even or odd). A simple binary string parity classification can be represented by recursive XOR operations, an ideal use-case for an LSTM&rsquo;s recurrent structure.</p>
<h4 id="univariate-lstm-setup">Univariate LSTM Setup<a hidden class="anchor" aria-hidden="true" href="#univariate-lstm-setup">#</a></h4>
<p>We can consider a LSTM where inputs, outputs, and weights are scalars, defined by:</p>
<p>$i_t = \sigma(w_{ix}x_t + w_{ih}h_{t-1} + b_i)$<br>
$f_t = \sigma(w_{fx}x_t + w_{fh}h_{t-1} + b_f)$<br>
$o_t = \sigma(w_{ox}x_t + w_{oh}h_{t-1} + b_o)$<br>
$g_t = \tanh(w_{gx}x_t + w_{gh}h_{t-1} + b_g)$<br>
$c_t = f_t c_{t-1} + i_t g_t$<br>
$h_t = o_t \tanh(c_t)$</p>
<h4 id="manual-parameter-setting-for-xor">Manual Parameter Setting for XOR<a hidden class="anchor" aria-hidden="true" href="#manual-parameter-setting-for-xor">#</a></h4>
<p>We can find weights and biases to perform parity classification manually. The goal was to have the final hidden state (<code>h_t</code>) ≥ 0.5 for odd parity and &lt; 0.5 for even parity. The selected weights and biases:</p>
<ul>
<li><strong>Input gate</strong>: <code>wix = 10</code>, <code>wih = 10</code>, <code>bi = -5</code></li>
<li><strong>Forget gate</strong>: <code>wfx = 0</code>, <code>wfh = 0</code>, <code>bf = -10</code></li>
<li><strong>Output gate</strong>: <code>wox = -10</code>, <code>woh = -10</code>, <code>bo = 15</code></li>
<li><strong>Gate <code>g</code></strong>: <code>wgx = 0</code>, <code>wgh = 0</code>, <code>bg = 10</code></li>
</ul>
<p>With these parameters:</p>
<ul>
<li><code>i_t</code> acts as an OR gate.</li>
<li><code>o_t</code> acts as a NAND gate.</li>
<li><code>c_t</code> effectively behaves as an AND gate.</li>
</ul>
<p>This demonstrates that even a minimal LSTM can solve the parity problem through careful manual configuration.</p>
<h4 id="understanding">Understanding<a hidden class="anchor" aria-hidden="true" href="#understanding">#</a></h4>
<p>We have demonstrated that a single-dimensional LSTM can theoretically compute parity for binary sequences of arbitrary length, setting the foundation to later explore learning these parameters automatically.</p>
<hr>
<h2 id="learning-finite-state-machines-with-lstm">Learning Finite State Machines with LSTM<a hidden class="anchor" aria-hidden="true" href="#learning-finite-state-machines-with-lstm">#</a></h2>
<p>This section explores the capacity of LSTMs to model deterministic finite state machines (FSMs), including both synthetic binary classification and structured language sequences.</p>
<h4 id="parity-task-generalization-to-longer-sequences">Parity Task: Generalization to Longer Sequences<a hidden class="anchor" aria-hidden="true" href="#parity-task-generalization-to-longer-sequences">#</a></h4>
<p>The LSTM is trained on binary sequences of varying lengths to predict their parity (even or odd number of 1s). The dataset is generated with all binary combinations up to a <code>max_length</code>, and the labels are calculated as <code>sum(seq) % 2</code>.</p>
<h4 id="model-summary">Model Summary<a hidden class="anchor" aria-hidden="true" href="#model-summary">#</a></h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">class</span> <span style="color:#0a0;text-decoration:underline">ParityLSTM</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> __init__(self, hidden_dim=<span style="color:#099">64</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#0aa">super</span>().__init__()
</span></span><span style="display:flex;"><span>        self.rnn = nn.LSTM(input_size=<span style="color:#099">1</span>, hidden_size=hidden_dim, batch_first=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>        self.fc = nn.Linear(hidden_dim, <span style="color:#099">2</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">forward</span>(self, x, lengths):
</span></span><span style="display:flex;"><span>        x = pack_padded_sequence(x, lengths, batch_first=<span style="color:#00a">True</span>, enforce_sorted=<span style="color:#00a">False</span>)
</span></span><span style="display:flex;"><span>        _, (h_n, _) = self.rnn(x)
</span></span><span style="display:flex;"><span>        output = self.fc(h_n[-<span style="color:#099">1</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> output
</span></span></code></pre></div><ul>
<li>Input is padded to batch format, and packed before passing to the LSTM.</li>
<li>Final hidden state is used for classification via a fully connected output layer.</li>
</ul>
<h4 id="understanding-1">Understanding<a hidden class="anchor" aria-hidden="true" href="#understanding-1">#</a></h4>
<div align="center">
<div style="display: flex; gap: 10px; justify-content: center; align-items: flex-start; flex-wrap: wrap;">
  <div style="text-align: center;">
    <img src="/finite-state-machine/LSTM-1_parity_generalization.png" width="230">
    <div>(a) Hidden size = 1</div>
  </div>
  <div style="text-align: center;">
    <img src="/finite-state-machine/LSTM-16_parity_generalization.png" width="230">
    <div>(b) Hidden size = 16</div>
  </div>
  <div style="text-align: center;">
    <img src="/finite-state-machine/LSTM-256_parity_generalization.png" width="230">
    <div>(c) Hidden size = 256</div>
  </div>
</div>
<p><strong>Figure 1</strong>: LSTM Parity Detection Accuracy with Varying Hidden Sizes</p>
</div>
<ul>
<li>LSTM with hidden size of <strong>1</strong> can learn the task to <strong>100% accuracy</strong>, validating the theoretical result.</li>
<li>Larger hidden sizes speed up convergence but may <strong>overfit</strong> on shorter training sequences.</li>
<li>Generalization to sequences up to <strong>length 256</strong> is tested, and performance drops slightly unless tuned properly.</li>
</ul>
<hr>
<h4 id="embedded-reber-grammar-erg-recognizing-structured-sequences">Embedded Reber Grammar (ERG): Recognizing Structured Sequences<a hidden class="anchor" aria-hidden="true" href="#embedded-reber-grammar-erg-recognizing-structured-sequences">#</a></h4>
<p>The LSTM is further challenged with a more complex task: classifying whether a string was generated by a structured <strong>Embedded Reber Grammar (ERG)</strong>.
To evaluate how recurrent models handle structured, long-range dependencies, we use the <strong>Embedded Reber Grammar (ERG)</strong> task. This synthetic task challenges a model to classify whether a given sequence follows the strict rules of ERG generation.</p>
<h4 id="what-is-the-embedded-reber-grammar">What is the Embedded Reber Grammar?<a hidden class="anchor" aria-hidden="true" href="#what-is-the-embedded-reber-grammar">#</a></h4>
<p>The <strong>Embedded Reber Grammar</strong> is a state machine used to generate sequences of characters following recursive, nested patterns. It contains two identical sub-networks (Reber grammars) that can repeat multiple times before the sequence ends.</p>
<ul>
<li>
<p>A valid ERG string example:<br>
<code>BTBTXSEBTXSEBPVVEBTXXVVETE</code></p>
<p>This decomposes into:<br>
<code>BT | BTXSE | BTXSE | BPVVE | BTXXVVE | TE</code></p>
</li>
<li>
<p>The task is to classify whether a given sequence is valid (follows ERG rules) or invalid (e.g., due to character-level perturbations).</p>
</li>
</ul>
<div align="center">
<img src="/finite-state-machine/erg.png" width="500">
<p><strong>Figure 2</strong>: ERG generation diagram</p>
</div>
<h4 id="models-compared">Models Compared<a hidden class="anchor" aria-hidden="true" href="#models-compared">#</a></h4>
<p>Two models are evaluated on this task:</p>
<table>
  <thead>
      <tr>
          <th>Model</th>
          <th>Train Accuracy</th>
          <th>Validation Generalization</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>RNN</td>
          <td>High</td>
          <td>Poor (overfits)</td>
      </tr>
      <tr>
          <td>LSTM</td>
          <td>High</td>
          <td>Strong generalization</td>
      </tr>
  </tbody>
</table>
<div align="center">
<img src="/finite-state-machine/graph.png" width="500">
<p><strong>Figure 3</strong>: RNN vs LSTM</p>
</div>
<ul>
<li><strong>RNN</strong>: Struggles with long-term dependencies, fails to generalize despite fitting the training set.</li>
<li><strong>LSTM</strong>: Learns the underlying recursive structure and performs well on unseen examples.</li>
</ul>
<h4 id="why-lstm-outperforms-rnn">Why LSTM Outperforms RNN?<a hidden class="anchor" aria-hidden="true" href="#why-lstm-outperforms-rnn">#</a></h4>
<p>LSTM&rsquo;s design includes key architectural features:</p>
<ul>
<li><strong>Input, forget, and output gates</strong> allow selective memory retention.</li>
<li><strong>Cell state</strong> enables long-distance signal propagation without degradation.</li>
<li><strong>Effective for recursion and repeated structures</strong>, unlike RNNs, which suffer from vanishing gradients.</li>
</ul>
<p>As a result, LSTMs can maintain context across complex, nested subsequences — which is essential for modeling grammars like ERG.</p>
<hr>
<h2 id="part-of-speech-tagging-with-bilstm">Part-of-Speech Tagging with BiLSTM<a hidden class="anchor" aria-hidden="true" href="#part-of-speech-tagging-with-bilstm">#</a></h2>
<p>This task applies BiLSTM models to a real-world NLP application — tagging each word in an English sentence with its corresponding part-of-speech (POS) using the <a href="https://universaldependencies.org/" target="_blank">UDPOS dataset</a>.</p>
<h4 id="dataset-overview">Dataset Overview<a hidden class="anchor" aria-hidden="true" href="#dataset-overview">#</a></h4>
<ul>
<li>Comes with <code>train</code>, <code>valid</code>, and <code>test</code> splits.</li>
<li>Includes a mix of topics (e.g., family, employment, science).</li>
<li>POS distribution is <strong>imbalanced</strong>, so majority label baseline is used as a sanity check.</li>
</ul>
<h4 id="preprocessing">Preprocessing<a hidden class="anchor" aria-hidden="true" href="#preprocessing">#</a></h4>
<ul>
<li>Custom <code>pad_collate()</code> is used to batch variable-length sequences.</li>
<li>Lemmatization is not applied, but could help reduce sparsity.</li>
<li>Words are converted to token IDs via a vocabulary object or <code>torchtext</code> pipeline.</li>
</ul>
<div align="center">
<img src="/finite-state-machine/Histogram.png" width="500">
<p><strong>Figure 4</strong>: POS Histogram</p>
</div>
<h4 id="bilstm-model-architecture">BiLSTM Model Architecture<a hidden class="anchor" aria-hidden="true" href="#bilstm-model-architecture">#</a></h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">class</span> <span style="color:#0a0;text-decoration:underline">BILSTM_POS</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> __init__(self, vocab_size, tag_size, embedding_dim=<span style="color:#099">128</span>, hidden_dim=<span style="color:#099">256</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#0aa">super</span>().__init__()
</span></span><span style="display:flex;"><span>        self.embedding = nn.Embedding(vocab_size, embedding_dim)
</span></span><span style="display:flex;"><span>        self.bilstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=<span style="color:#00a">True</span>, batch_first=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>        self.dropout = nn.Dropout(<span style="color:#099">0.5</span>)
</span></span><span style="display:flex;"><span>        self.fc = nn.Linear(hidden_dim * <span style="color:#099">2</span>, tag_size)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">forward</span>(self, x, lengths):
</span></span><span style="display:flex;"><span>        x = self.embedding(x)
</span></span><span style="display:flex;"><span>        x = pack_padded_sequence(x, lengths, batch_first=<span style="color:#00a">True</span>, enforce_sorted=<span style="color:#00a">False</span>)
</span></span><span style="display:flex;"><span>        o, _ = self.bilstm(x)
</span></span><span style="display:flex;"><span>        o, _ = pad_packed_sequence(o, batch_first=<span style="color:#00a">True</span>)
</span></span><span style="display:flex;"><span>        o = self.dropout(o)
</span></span><span style="display:flex;"><span>        o = self.fc(o)
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> torch.log_softmax(o, dim=-<span style="color:#099">1</span>)
</span></span></code></pre></div><ul>
<li>Embedding layer → BiLSTM → dropout → linear output → log-softmax over tags</li>
<li>Bidirectional structure ensures each word is contextualized with both left and right neighbors.</li>
</ul>
<h4 id="training-observations">Training Observations<a hidden class="anchor" aria-hidden="true" href="#training-observations">#</a></h4>
<ul>
<li>Training accuracy improves steadily and outpaces validation loss after ~30 epochs.</li>
<li>Likely due to over-representation of common tokens like <code>UNK</code>, which default to the <code>NOUN</code> tag early in training.</li>
<li>Dropout regularization helps mitigate overfitting.</li>
</ul>
<div align="center">
<img src="/finite-state-machine/Loss.png" width="500">
<p><strong>Figure 5</strong>: Train and Validation Loss</p>
</div>
<h4 id="loss-trend">Loss Trend<a hidden class="anchor" aria-hidden="true" href="#loss-trend">#</a></h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Epoch 40/40
</span></span><span style="display:flex;"><span>Train Loss: 0.0227
</span></span><span style="display:flex;"><span>Valid Loss: 0.2679
</span></span><span style="display:flex;"><span>Test Accuracy: 86.23%
</span></span></code></pre></div><h4 id="pos-tagging-inference-examples">POS Tagging Inference Examples<a hidden class="anchor" aria-hidden="true" href="#pos-tagging-inference-examples">#</a></h4>
<p><strong>Example 1:</strong><br>
<code>The old man the boat.</code><br>
<code>DET ADJ NOUN DET NOUN PUNCT</code></p>
<p><strong>Example 2:</strong><br>
<code>The complex houses married and single soldiers and their families.</code><br>
<code>DET ADJ NOUN VERB CCONJ ADJ NOUN CCONJ PRON NOUN PUNCT</code></p>
<p><strong>Example 3:</strong><br>
<code>The man who hunts ducks out on weekends.</code><br>
<code>DET NOUN PRON PROPN VERB ADV ADP NOUN PUNCT</code></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/nlp/">NLP</a></li>
      <li><a href="http://localhost:1313/tags/lstm/">LSTM</a></li>
      <li><a href="http://localhost:1313/tags/rnn/">RNN</a></li>
      <li><a href="http://localhost:1313/tags/sequence-modeling/">Sequence Modeling</a></li>
      <li><a href="http://localhost:1313/tags/machine-learning/">Machine Learning</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Yong-Hwan Lee</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
