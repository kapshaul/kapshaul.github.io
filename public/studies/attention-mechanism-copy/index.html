<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Natural Language Processing (NLP) - Attention Mechanisms in Sequence-to-Sequence Models | Yong-Hwan Lee</title>
<meta name="keywords" content="NLP, Machine Learning, Deep Learning, Attention Mechanism, Transformer, BLEU Score, ArtificialIntelligence">
<meta name="description" content="This study was carried out as a project at Oregon State University.">
<meta name="author" content="Yong-Hwan Lee">
<link rel="canonical" href="http://localhost:1313/studies/attention-mechanism-copy/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d6cf4a8fa527330d9574f36d8d000fdaf90ca838ff09ab72fc27d3cb7ca1ddc5.css" integrity="sha256-1s9Kj6UnMw2VdPNtjQAP2vkMqDj/Caty/CfTy3yh3cU=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/studies/attention-mechanism-copy/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="Natural Language Processing (NLP) - Attention Mechanisms in Sequence-to-Sequence Models" />
<meta property="og:description" content="This study was carried out as a project at Oregon State University." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/studies/attention-mechanism-copy/" /><meta property="article:section" content="studies" />
<meta property="article:published_time" content="2024-06-07T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-07-12T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Natural Language Processing (NLP) - Attention Mechanisms in Sequence-to-Sequence Models"/>
<meta name="twitter:description" content="This study was carried out as a project at Oregon State University."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Studies",
      "item": "http://localhost:1313/studies/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Natural Language Processing (NLP) - Attention Mechanisms in Sequence-to-Sequence Models",
      "item": "http://localhost:1313/studies/attention-mechanism-copy/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Natural Language Processing (NLP) - Attention Mechanisms in Sequence-to-Sequence Models",
  "name": "Natural Language Processing (NLP) - Attention Mechanisms in Sequence-to-Sequence Models",
  "description": "This study was carried out as a project at Oregon State University.",
  "keywords": [
    "NLP", "Machine Learning", "Deep Learning", "Attention Mechanism", "Transformer", "BLEU Score", "ArtificialIntelligence"
  ],
  "articleBody": "Overview The main objective of this project is to understand scaled dot-product attention and to implement a simple attention mechanism in a sequence-to-sequence model. The task involves translating sentences from German to English using the Multi30k dataset, which contains over 31,000 bitext sentences describing common visual scenes in both languages.\nScaled Dot-Product Attention Starts from the definition of a single-query scaled dot-product attention mechanism. Given a query $\\mathbf{q} \\in \\mathbb{R}^{1\\times d}$, a set of candidates represented by keys $\\mathbf{k}_1, … , \\mathbf{k}_m \\in \\mathbb{R}^{1\\times d}$ and values $\\mathbf{v}_1, … , \\mathbf{v}_m \\in \\mathbb{R}^{1\\times d_v}$, we compute the scaled dot-product attention as:\n$$ \\alpha_i = \\frac{\\text{exp}\\left(~\\mathbf{q}\\mathbf{k}_ i^T / \\sqrt d\\right)}{\\sum_{j=1}^m \\text{exp}\\left(\\mathbf{q}\\mathbf{k}_ j^T / \\sqrt d\\right)} $$\n$$ \\textbf{a} = \\sum_{j=1}^m \\alpha_j \\mathbf{v}_j $$\nwhere the $\\alpha_i$ are referred to as attention values (or collectively as an attention distribution).\n1.1. Copying Q. Describe what properties of the keys and queries would result in the output $\\textbf{a}$ being equal to one of the input values $\\mathbf{v}_j$. Specifically, what must be true about the query $\\mathbf{q}$ and the keys $\\mathbf{k}_1, …, \\mathbf{k}_m$ such that $\\textbf{a} \\approx \\mathbf{v}_j$? (We assume all values are unique – $\\mathbf{v}_i \\neq \\mathbf{v}_j,~\\forall ij$.)\nIn the case where $\\textbf{a} \\approx \\mathbf{v}_j$, the similarity score for $\\mathbf{q}\\mathbf{k}_j^T$ is significantly higher than all others due to the softmax function producing outputs to have probability distribution. Therefore, given a query $\\mathbf{q}$, $\\mathbf{k}_j$ must be significantly higher than others to determine the similarity score.\n1.2. Average of Two Q. Consider a set of key vectors $\\mathbf{k}_1, … , \\mathbf{k}_m$ where all keys are orthogonal unit vectors – that is to say $\\mathbf{k}_i \\mathbf{k}_j^T = 0, \\forall ij$ and $\\Vert\\mathbf{k}_i\\Vert=1,\\forall i$. Let $\\mathbf{v}_a, \\mathbf{v}_b \\in {\\mathbf{v}_1, …, \\mathbf{v}_m}$ be two value vectors. Give an expression for a query vector $\\mathbf{q}$ such that the output $\\textbf{a}$ is approximately equal to the average of $\\mathbf{v}_a$ and $\\mathbf{v}_b$, that is to say $\\textbf{a} \\approx \\frac{1}{2}(\\mathbf{v}_a + \\mathbf{v}_b)$. You can reference the key vectors corresponding to $\\mathbf{v}_a$ and $\\mathbf{v}_b$ as $\\mathbf{k}_a$ and $\\mathbf{k_b}$ respectively.\nFrom $\\textbf{a} \\approx \\frac{1}{2}(\\mathbf{v}_a + \\mathbf{v}_b)$, we can consider the term $\\frac{1}{2}$ is from $\\alpha_i$. Meaning that $\\alpha_a = \\alpha_b$ and $\\alpha_i = 0$ should be satisfied to meet the condition. Since $\\alpha_i = \\text{softmax}(\\mathbf{q}\\mathbf{k}_i^T)$, we only want to keep $\\mathbf{k}_a$ and $\\mathbf{k}_b$; otherwise, $\\mathbf{k}_i=0$.\nBy constructing $\\mathbf{q}=\\mathbf{k}_a + \\mathbf{k}_b$, we can ensure if this expression satisfy the condition.\n$$ \\mathbf{q}\\mathbf{k}_a^T=(\\mathbf{k}_a\\mathbf{k}_a^T + \\mathbf{k}_b\\mathbf{k}_a^T)=(1+0)=1 $$\n$$ \\mathbf{q}\\mathbf{k}_b^T=(\\mathbf{k}_a\\mathbf{k}_b^T + \\mathbf{k}_b\\mathbf{k}_b^T)=(0+1)=1 $$\n$$ \\mathbf{q}\\mathbf{k}_i^T=(\\mathbf{k}_a\\mathbf{k}_i^T + \\mathbf{k}_b\\mathbf{k}_i^T)=(0+0)=0 $$\nAfter applying the softmax function, given that $\\mathbf{q}\\mathbf{k}_a^T = 1$, $\\mathbf{q}\\mathbf{k}_b^T = 1$, and $\\mathbf{q}\\mathbf{k}_i^T = 0$, the resulting attention weights are approximately $\\alpha_a \\approx \\frac{1}{2}$ and $\\alpha_b \\approx \\frac{1}{2}$. Therefore $\\textbf{a}$ can be written,\n$$ \\textbf{a} \\approx \\frac{1}{2}\\mathbf{v}_a + \\frac{1}{2}\\mathbf{v}_b + 0 $$\n$$ \\textbf{a} \\approx \\frac{1}{2}(\\mathbf{v}_a + \\mathbf{v}_b) $$\n1.3. Noisy Average Q. Now consider a set of key vectors ${\\mathbf{k}_1, … , \\mathbf{k}_m}$ where keys are randomly scaled such that $\\mathbf{k}_i = \\mathbf{\\mu}_i*\\lambda_i$ where $\\lambda_i \\sim \\mathcal{N}(1, \\beta)$ is a randomly sampled scalar multiplier. Assume the unscaled vectors $\\mu_1, …, \\mu_m$ are orthogonal unit vectors. If you use the same strategy to construct the query $q$ as you did in Task 1.2, what would be the outcome here? Specifically, derive $\\mathbf{q}\\mathbf{k}_a^T$ and $\\mathbf{q}\\mathbf{k}_b^T$ in terms of $\\mu$’s and $\\lambda$’s. Qualitatively describe what how the output $a$ would vary over multiple resamplings of $\\lambda_1, …, \\lambda_m$.\nFrom the expression for $\\mathbf{q}$ in Task 1.2,\n$$ \\mathbf{q}=\\mathbf{k}_a + \\mathbf{k}_b $$\nBy substituting $\\mathbf{k}_i = \\mathbf{\\mu}_i*\\lambda_i$,\n$$ \\mathbf{q}=\\mathbf{\\mu}_a*\\lambda_a + \\mathbf{\\mu}_b*\\lambda_b $$\nThe expression for $\\mathbf{q}\\mathbf{k}_a^T$ and $\\mathbf{q}\\mathbf{k}_b^T$,\n$$ \\mathbf{q}\\mathbf{k}_a^T=\\lambda_a^2*\\mathbf{\\mu}_a\\mathbf{\\mu}_a^T + \\lambda_a\\lambda_b*\\mathbf{\\mu}_b\\mathbf{\\mu}_a^T)=\\lambda_a^2 $$\n$$ \\mathbf{q}\\mathbf{k}_b^T=(\\lambda_a\\lambda_b*\\mathbf{\\mu}_a\\mathbf{\\mu}_b^T + \\lambda_b^2*\\mathbf{\\mu}_b\\mathbf{\\mu}_b^T)=\\lambda_b^2 $$\nNow, we can consider 3 different cases,\nCase 1. $\\lambda_a \\approx \\lambda_b$,\n$$ \\textbf{a} \\approx \\frac{1}{2}(\\mathbf{v}_a + \\mathbf{v}_b) $$\nCase 2. $\\lambda_a \\gg \\lambda_b$,\n$$ \\textbf{a} \\approx \\mathbf{v}_a $$\nCase 3. $\\lambda_a \\ll \\lambda_b$,\n$$ \\textbf{a} \\approx \\mathbf{v}_b $$\nSince randomly sampled following $\\lambda_i \\sim \\mathcal{N}(1, \\beta)$,\n$$ \\mathbb{E}[\\mathbf{q}\\mathbf{k}_a^T]=\\mathbb{E}[\\lambda_a^2]=1 $$\n$$ \\mathbb{E}[\\mathbf{q}\\mathbf{k}_b^T]=\\mathbb{E}[\\lambda_b^2]=1 $$\nOver multiple resamplings of $\\lambda_1, …, \\lambda_m$,\n$$ \\mathbb{E}[\\textbf{a}] = \\frac{1}{2}(\\mathbf{v}_a + \\mathbf{v}_b) $$\n1.4. Noisy Average with Multi-head Attention Q. Let’s now consider a simple version of multi-head attention that averages the attended features resulting from two different queries. Here, two queries are defined ($\\mathbf{q}_1$ and $\\mathbf{q}_2$) leading to two different attended features ($\\textbf{a}_1$ and $\\textbf{a}_2$). The output of this computation will be $\\textbf{a} = \\frac{1}{2}(\\textbf{a}_1 + \\textbf{a}_2)$. Assume we have keys like those in Task 1.3, design queries $\\mathbf{q}_1$ and $\\mathbf{q}_2$ such that $\\textbf{a} \\approx \\frac{1}{2}(\\mathbf{v}_a + \\mathbf{v}_b)$.\nA simple strategy is to design each query so that one head selects $q_1=\\mathbf{v}_a$ exclusively and the other head selects $q_2=\\mathbf{v}_b$ exclusively. Then, by averaging those two attended features,\n$$ \\mathbf{q}_1=\\lambda_a*\\mathbf{\\mu}_a $$\n$$ \\mathbf{q}_2=\\lambda_b*\\mathbf{\\mu}_b $$\nBy constructing the linear equation of $\\mathbf{q}$ and $\\mathbf{k}$,\n$$ \\mathbf{q}_1\\mathbf{k}_a^T=\\lambda_a*\\mathbf{\\mu}_a\\mathbf{k}_a^T =\\lambda_a^2*(\\mathbf{\\mu}_a\\mathbf{\\mu}_a^T)=\\lambda_a^2 $$\n$$ \\mathbf{q}_2\\mathbf{k}_b^T=\\lambda_b*\\mathbf{\\mu}_b\\mathbf{k}_b^T =\\lambda_b^2*(\\mathbf{\\mu}_b\\mathbf{\\mu}_b^T)=\\lambda_b^2 $$\nFrom here, $\\textbf{a}_1$ and $\\textbf{a}_2$ can be expressed,\n$$ \\alpha_i=\\text{softmax}(\\mathbf{q}_1\\mathbf{k}_i^T) \\hspace{20pt} \\alpha_j=\\text{softmax}(\\mathbf{q}_2\\mathbf{k}_j^T) $$\n$$ \\alpha_i = \\begin{cases} 1 \u0026 \\text{if } i = a \\ 0 \u0026 \\text{if } i \\neq a \\end{cases} \\hspace{20pt} \\alpha_j = \\begin{cases} 1 \u0026 \\text{if } j = b \\ 0 \u0026 \\text{if } j \\neq b \\end{cases} $$\n$$ \\textbf{a}_1 = \\alpha_a \\mathbf{v}_a = \\mathbf{v}_a \\hspace{40pt} \\textbf{a}_2 = \\alpha_b \\mathbf{v}_b = \\mathbf{v}_b $$\nThe final output is the average of $\\textbf{a}_1$ and $\\textbf{a}_2$.\n$$ \\textbf{a} = \\frac{1}{2}(\\textbf{a}_1 + \\textbf{a}_2) $$\nGerman-to-English Machine Translation Machine translation involves automatically converting text from one language to another using computational models. In the context of translating German to English, Scaled-Dot Product Attention is a core mechanism used in modern neural networks, particularly in models like the Transformer. This mechanism allows the model to focus on relevant parts of the input sentence when generating the output translation, enabling it to capture the nuances and structure of German and translate them effectively into English.\nBy using Scaled-Dot Product Attention, the model effectively learns these common patterns and differences between German and English\n2.1. German-to-English Attention Pattern Example 1 Example 2 Example 3 Example 4 Figure 1: Attention diagram examples\nSVO vs SOV English typically follows an SVO (Subject-Verb-Object) structure, while German often employs an SOV (Subject-Object-Verb) structure in subordinate clauses. This difference leads to deviations from the diagonal in the verb area on attention maps, as illustrated in Examples 1 and 2. For instance, deviations can be seen in the phrases “are climbing” in Example 1 and “are anticipating” in Example 2.\nPreposition and Conjunction Prepositions and their objects usually align closely, but their usage can differ in certain cases. For example, in Example 3, the English preposition “to” corresponds to the German preposition “zu,” and the English conjunction “while” corresponds to the German conjunction “während.” Both of these cases show deviations rather than diagonal alignment. In Example 4, the English preposition “in front of” corresponds to the German preposition “vor,” which also shows significant deviation.\n2.2. Scaled-Dot Product Attention Code class SingleQueryScaledDotProductAttention(nn.Module): # kq_dim is the dimension of keys and queries. Linear layers should be used to project inputs to these dimensions. def __init__(self, enc_hid_dim, dec_hid_dim, kq_dim=512): super().__init__() self.W_k = nn.Linear(enc_hid_dim * 2, kq_dim) self.W_q = nn.Linear(dec_hid_dim, kq_dim) self.kq_dim = kq_dim #hidden is h_t^{d} from Eq. (11) and has dim =\u003e [batch_size, dec_hid_dim] #encoder_outputs is the word representations from Eq. (6) # and has dim =\u003e [src_len, batch_size, enc_hid_dim * 2] def forward(self, hidden, encoder_outputs): # Compute for q = W_q h_j^{e} query = self.W_q(hidden).unsqueeze(1) # Compute for k_t = W_k h_t^{d} key = self.W_k(encoder_outputs) # Compute for score = qk^T / sqrt(d) score = torch.bmm(query, key.T(1, 2)).squeeze(1) / np.sqrt(self.kq_dim) # Compute alpha = softmax(score) alpha = F.softmax(score, dim=1) # Compute a = sum(alpha_j v_j) attended_val = torch.sum(encoder_outputs * alpha.unsqueeze(2), dim=1) assert attended_val.shape == (hidden.shape[0], encoder_outputs.shape[2]) assert alpha.shape == (hidden.shape[0], encoder_outputs.shape[0]) return attended_val, alpha 2.3. BLEU Score Comparision Models were trained and evaluated using the Dummy baseline, MeanPool, and attention mechanisms. For each method, three independent runs were conducted, and the mean and variance of the results were calculated to compare performance and consistency.\nAttn. PPL Mean PPL Var BLEU Mean BLEU Var None 18.107 0.057 16.3 0.118 Mean 15.809 0.028 18.599 0.277 SDP 10.447 0.049 34.64 0.793 The trends indicate that the scaled dot-product attention mechanism (SDP), significantly improve the model’s performance, as lower perplexity and higher BLEU scores. However, the increased variance in BLEU scores with SDP suggests that there might be some variability caused by random initialization.\n",
  "wordCount" : "1382",
  "inLanguage": "en",
  "datePublished": "2024-06-07T00:00:00Z",
  "dateModified": "2024-07-12T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Yong-Hwan Lee"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/studies/attention-mechanism-copy/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Yong-Hwan Lee",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class=" dark" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Yong-Hwan Lee">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Yong-Hwan Lee</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/studies/" title="Studies">
                    <span>Studies</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Natural Language Processing (NLP) - Attention Mechanisms in Sequence-to-Sequence Models
    </h1>
    <div class="post-meta"><span title='2024-06-07 00:00:00 +0000 UTC'>June 2024</span>&nbsp;&middot;&nbsp;Yong-Hwan Lee&nbsp;&middot;&nbsp;<a href="https://github.com/kapshaul/NLP-attention.mechanism" rel="noopener noreferrer" target="_blank">GitHub</a>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><nav id="TableOfContents">
  <ul>
    <li><a href="#overview">Overview</a></li>
    <li><a href="#scaled-dot-product-attention">Scaled Dot-Product Attention</a></li>
    <li><a href="#german-to-english-machine-translation">German-to-English Machine Translation</a></li>
  </ul>
</nav>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="overview">Overview<a hidden class="anchor" aria-hidden="true" href="#overview">#</a></h2>
<p>The main objective of this project is to understand scaled dot-product attention and to implement a simple attention mechanism in a sequence-to-sequence model. The task involves translating sentences from German to English using the Multi30k dataset, which contains over 31,000 bitext sentences describing common visual scenes in both languages.</p>
<hr>
<h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention<a hidden class="anchor" aria-hidden="true" href="#scaled-dot-product-attention">#</a></h2>
<p>Starts from the definition of a single-query scaled dot-product attention mechanism. Given a query $\mathbf{q} \in \mathbb{R}^{1\times d}$, a set of candidates represented by keys $\mathbf{k}_1, &hellip; , \mathbf{k}_m \in \mathbb{R}^{1\times d}$ and values $\mathbf{v}_1, &hellip; , \mathbf{v}_m \in \mathbb{R}^{1\times d_v}$, we compute the scaled dot-product attention as:</p>
<p>$$
\alpha_i = \frac{\text{exp}\left(~\mathbf{q}\mathbf{k}_ i^T / \sqrt d\right)}{\sum_{j=1}^m \text{exp}\left(\mathbf{q}\mathbf{k}_ j^T / \sqrt d\right)}
$$</p>
<p>$$
\textbf{a} = \sum_{j=1}^m \alpha_j \mathbf{v}_j
$$</p>
<p>where the $\alpha_i$ are referred to as attention values (or collectively as an attention distribution).</p>
<h4 id="11-copying"><strong>1.1. Copying</strong><a hidden class="anchor" aria-hidden="true" href="#11-copying">#</a></h4>
<p>Q. Describe what properties of the keys and queries would result in the output $\textbf{a}$ being equal to one of the input values $\mathbf{v}_j$. Specifically, what must be true about the query $\mathbf{q}$ and the keys $\mathbf{k}_1, &hellip;, \mathbf{k}_m$ such that $\textbf{a} \approx \mathbf{v}_j$? (We assume all values are unique &ndash; $\mathbf{v}_i \neq \mathbf{v}_j,~\forall ij$.)</p>
<blockquote>
<p>In the case where $\textbf{a} \approx \mathbf{v}_j$, the similarity score for $\mathbf{q}\mathbf{k}_j^T$ is significantly higher than all others due to the softmax function producing outputs to have probability distribution. Therefore, given a query $\mathbf{q}$, $\mathbf{k}_j$ must be significantly higher than others to determine the similarity score.</p></blockquote>
<h4 id="12-average-of-two"><strong>1.2. Average of Two</strong><a hidden class="anchor" aria-hidden="true" href="#12-average-of-two">#</a></h4>
<p>Q. Consider a set of key vectors $\mathbf{k}_1, &hellip; , \mathbf{k}_m$ where all keys are orthogonal unit vectors &ndash; that is to say $\mathbf{k}_i \mathbf{k}_j^T = 0, \forall ij$ and $\Vert\mathbf{k}_i\Vert=1,\forall i$. Let $\mathbf{v}_a, \mathbf{v}_b \in {\mathbf{v}_1, &hellip;, \mathbf{v}_m}$ be two value vectors. Give an expression for a query vector $\mathbf{q}$ such that the output $\textbf{a}$ is approximately equal to the average of $\mathbf{v}_a$ and $\mathbf{v}_b$, that is to say $\textbf{a} \approx \frac{1}{2}(\mathbf{v}_a + \mathbf{v}_b)$. You can reference the key vectors corresponding to $\mathbf{v}_a$ and $\mathbf{v}_b$ as $\mathbf{k}_a$ and $\mathbf{k_b}$ respectively.</p>
<blockquote>
<p>From $\textbf{a} \approx \frac{1}{2}(\mathbf{v}_a + \mathbf{v}_b)$, we can consider the term $\frac{1}{2}$ is from $\alpha_i$. Meaning that $\alpha_a = \alpha_b$ and $\alpha_i = 0$ should be satisfied to meet the condition. Since $\alpha_i = \text{softmax}(\mathbf{q}\mathbf{k}_i^T)$, we only want to keep $\mathbf{k}_a$ and $\mathbf{k}_b$; otherwise, $\mathbf{k}_i=0$.</p>
<p>By constructing $\mathbf{q}=\mathbf{k}_a + \mathbf{k}_b$, we can ensure if this expression satisfy the condition.</p>
<p>$$
\mathbf{q}\mathbf{k}_a^T=(\mathbf{k}_a\mathbf{k}_a^T + \mathbf{k}_b\mathbf{k}_a^T)=(1+0)=1
$$</p>
<p>$$
\mathbf{q}\mathbf{k}_b^T=(\mathbf{k}_a\mathbf{k}_b^T + \mathbf{k}_b\mathbf{k}_b^T)=(0+1)=1
$$</p>
<p>$$
\mathbf{q}\mathbf{k}_i^T=(\mathbf{k}_a\mathbf{k}_i^T + \mathbf{k}_b\mathbf{k}_i^T)=(0+0)=0
$$</p>
<p>After applying the softmax function, given that $\mathbf{q}\mathbf{k}_a^T = 1$, $\mathbf{q}\mathbf{k}_b^T = 1$, and $\mathbf{q}\mathbf{k}_i^T = 0$, the resulting attention weights are approximately $\alpha_a \approx \frac{1}{2}$ and $\alpha_b \approx \frac{1}{2}$.
Therefore $\textbf{a}$ can be written,</p>
<p>$$
\textbf{a} \approx \frac{1}{2}\mathbf{v}_a + \frac{1}{2}\mathbf{v}_b + 0
$$</p>
<p>$$
\textbf{a} \approx \frac{1}{2}(\mathbf{v}_a + \mathbf{v}_b)
$$</p></blockquote>
<h4 id="13-noisy-average"><strong>1.3. Noisy Average</strong><a hidden class="anchor" aria-hidden="true" href="#13-noisy-average">#</a></h4>
<p>Q. Now consider a set of key vectors ${\mathbf{k}_1, &hellip; , \mathbf{k}_m}$ where keys are randomly scaled such that $\mathbf{k}_i = \mathbf{\mu}_i*\lambda_i$ where $\lambda_i \sim \mathcal{N}(1, \beta)$ is a randomly sampled scalar multiplier. Assume the unscaled vectors $\mu_1, &hellip;, \mu_m$ are orthogonal unit vectors. If you use the same strategy to construct the query $q$ as you did in Task 1.2, what would be the outcome here? Specifically, derive $\mathbf{q}\mathbf{k}_a^T$ and $\mathbf{q}\mathbf{k}_b^T$ in terms of $\mu$&rsquo;s and $\lambda$&rsquo;s. Qualitatively describe what how the output $a$ would vary over multiple resamplings of $\lambda_1, &hellip;, \lambda_m$.</p>
<blockquote>
<p>From the expression for $\mathbf{q}$ in Task 1.2,</p>
<p>$$
\mathbf{q}=\mathbf{k}_a + \mathbf{k}_b
$$</p>
<p>By substituting $\mathbf{k}_i = \mathbf{\mu}_i*\lambda_i$,</p>
<p>$$
\mathbf{q}=\mathbf{\mu}_a*\lambda_a + \mathbf{\mu}_b*\lambda_b
$$</p>
<p>The expression for $\mathbf{q}\mathbf{k}_a^T$ and $\mathbf{q}\mathbf{k}_b^T$,</p>
<p>$$
\mathbf{q}\mathbf{k}_a^T=\lambda_a^2*\mathbf{\mu}_a\mathbf{\mu}_a^T + \lambda_a\lambda_b*\mathbf{\mu}_b\mathbf{\mu}_a^T)=\lambda_a^2
$$</p>
<p>$$
\mathbf{q}\mathbf{k}_b^T=(\lambda_a\lambda_b*\mathbf{\mu}_a\mathbf{\mu}_b^T + \lambda_b^2*\mathbf{\mu}_b\mathbf{\mu}_b^T)=\lambda_b^2
$$</p>
<p>Now, we can consider 3 different cases,</p>
<p>Case 1. $\lambda_a \approx \lambda_b$,</p>
<p>$$
\textbf{a} \approx \frac{1}{2}(\mathbf{v}_a + \mathbf{v}_b)
$$</p>
<p>Case 2. $\lambda_a \gg \lambda_b$,</p>
<p>$$
\textbf{a} \approx \mathbf{v}_a
$$</p>
<p>Case 3. $\lambda_a \ll \lambda_b$,</p>
<p>$$
\textbf{a} \approx \mathbf{v}_b
$$</p>
<p>Since randomly sampled following $\lambda_i \sim \mathcal{N}(1, \beta)$,</p>
<p>$$
\mathbb{E}[\mathbf{q}\mathbf{k}_a^T]=\mathbb{E}[\lambda_a^2]=1
$$</p>
<p>$$
\mathbb{E}[\mathbf{q}\mathbf{k}_b^T]=\mathbb{E}[\lambda_b^2]=1
$$</p>
<p>Over multiple resamplings of $\lambda_1, &hellip;, \lambda_m$,</p>
<p>$$
\mathbb{E}[\textbf{a}] = \frac{1}{2}(\mathbf{v}_a + \mathbf{v}_b)
$$</p></blockquote>
<h4 id="14-noisy-average-with-multi-head-attention"><strong>1.4. Noisy Average with Multi-head Attention</strong><a hidden class="anchor" aria-hidden="true" href="#14-noisy-average-with-multi-head-attention">#</a></h4>
<p>Q. Let&rsquo;s now consider a simple version of multi-head attention that averages the attended features resulting from two different queries. Here, two queries are defined ($\mathbf{q}_1$ and $\mathbf{q}_2$) leading to two different attended features ($\textbf{a}_1$ and $\textbf{a}_2$). The output of this computation will be $\textbf{a} = \frac{1}{2}(\textbf{a}_1 + \textbf{a}_2)$. Assume we have keys like those in Task 1.3, design queries $\mathbf{q}_1$ and $\mathbf{q}_2$ such that $\textbf{a} \approx \frac{1}{2}(\mathbf{v}_a + \mathbf{v}_b)$.</p>
<blockquote>
<p>A simple strategy is to design each query so that one head selects $q_1=\mathbf{v}_a$ exclusively and the other head selects $q_2=\mathbf{v}_b$ exclusively. Then, by averaging those two attended features,</p>
<p>$$
\mathbf{q}_1=\lambda_a*\mathbf{\mu}_a
$$</p>
<p>$$
\mathbf{q}_2=\lambda_b*\mathbf{\mu}_b
$$</p>
<p>By constructing the linear equation of $\mathbf{q}$ and $\mathbf{k}$,</p>
<p>$$
\mathbf{q}_1\mathbf{k}_a^T=\lambda_a*\mathbf{\mu}_a\mathbf{k}_a^T =\lambda_a^2*(\mathbf{\mu}_a\mathbf{\mu}_a^T)=\lambda_a^2
$$</p>
<p>$$
\mathbf{q}_2\mathbf{k}_b^T=\lambda_b*\mathbf{\mu}_b\mathbf{k}_b^T =\lambda_b^2*(\mathbf{\mu}_b\mathbf{\mu}_b^T)=\lambda_b^2
$$</p>
<p>From here, $\textbf{a}_1$ and $\textbf{a}_2$ can be expressed,</p>
<p>$$
\alpha_i=\text{softmax}(\mathbf{q}_1\mathbf{k}_i^T)
\hspace{20pt}
\alpha_j=\text{softmax}(\mathbf{q}_2\mathbf{k}_j^T)
$$</p>
<p>$$
\alpha_i =
\begin{cases}
1 &amp; \text{if } i = a \
0 &amp; \text{if } i \neq a
\end{cases}
\hspace{20pt}
\alpha_j =
\begin{cases}
1 &amp; \text{if } j = b \
0 &amp; \text{if } j \neq b
\end{cases}
$$</p>
<p>$$
\textbf{a}_1 = \alpha_a \mathbf{v}_a = \mathbf{v}_a
\hspace{40pt}
\textbf{a}_2 = \alpha_b \mathbf{v}_b = \mathbf{v}_b
$$</p>
<p>The final output is the average of $\textbf{a}_1$ and $\textbf{a}_2$.</p>
<p>$$
\textbf{a} = \frac{1}{2}(\textbf{a}_1 + \textbf{a}_2)
$$</p></blockquote>
<hr>
<h2 id="german-to-english-machine-translation">German-to-English Machine Translation<a hidden class="anchor" aria-hidden="true" href="#german-to-english-machine-translation">#</a></h2>
<p>Machine translation involves automatically converting text from one language to another using computational models. In the context of translating German to English, Scaled-Dot Product Attention is a core mechanism used in modern neural networks, particularly in models like the <strong>Transformer</strong>. This mechanism allows the model to focus on relevant parts of the input sentence when generating the output translation, enabling it to capture the nuances and structure of German and translate them effectively into English.</p>
<p>By using Scaled-Dot Product Attention, the model effectively learns these common patterns and differences between German and English</p>
<br>
<h4 id="21-german-to-english-attention-pattern"><strong>2.1. German-to-English Attention Pattern</strong><a hidden class="anchor" aria-hidden="true" href="#21-german-to-english-attention-pattern">#</a></h4>
<div align="center">
<p><strong>Example 1</strong>
<img src="/attention-mechanism/104_translation.png" width="300">
<strong>Example 2</strong>
<img src="/attention-mechanism/114_translation.png" width="300"></p>
<p><strong>Example 3</strong>
<img src="/attention-mechanism/281_translation.png" width="300">
<strong>Example 4</strong>
<img src="/attention-mechanism/759_translation.png" width="300"></p>
<p><strong>Figure 1</strong>: Attention diagram examples</p>
</div>
<ul>
<li><strong>SVO vs SOV</strong></li>
</ul>
<p>English typically follows an SVO (Subject-Verb-Object) structure, while German often employs an SOV (Subject-Object-Verb) structure in subordinate clauses. This difference leads to deviations from the diagonal in the verb area on attention maps, as illustrated in Examples 1 and 2. For instance, deviations can be seen in the phrases &ldquo;are climbing&rdquo; in Example 1 and &ldquo;are anticipating&rdquo; in Example 2.</p>
<ul>
<li><strong>Preposition and Conjunction</strong></li>
</ul>
<p>Prepositions and their objects usually align closely, but their usage can differ in certain cases. For example, in Example 3, the English preposition &ldquo;to&rdquo; corresponds to the German preposition &ldquo;zu,&rdquo; and the English conjunction &ldquo;while&rdquo; corresponds to the German conjunction &ldquo;während.&rdquo; Both of these cases show deviations rather than diagonal alignment. In Example 4, the English preposition &ldquo;in front of&rdquo; corresponds to the German preposition &ldquo;vor,&rdquo; which also shows significant deviation.</p>
<br>
<h4 id="22-scaled-dot-product-attention-code"><strong>2.2. Scaled-Dot Product Attention Code</strong><a hidden class="anchor" aria-hidden="true" href="#22-scaled-dot-product-attention-code">#</a></h4>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">class</span> <span style="color:#0a0;text-decoration:underline">SingleQueryScaledDotProductAttention</span>(nn.Module):    
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># kq_dim is the dimension of keys and queries. Linear layers should be used to project inputs to these dimensions.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> __init__(self, enc_hid_dim, dec_hid_dim, kq_dim=<span style="color:#099">512</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#0aa">super</span>().__init__()
</span></span><span style="display:flex;"><span>        self.W_k = nn.Linear(enc_hid_dim * <span style="color:#099">2</span>, kq_dim)
</span></span><span style="display:flex;"><span>        self.W_q = nn.Linear(dec_hid_dim, kq_dim)
</span></span><span style="display:flex;"><span>        self.kq_dim = kq_dim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic">#hidden is h_t^{d} from Eq. (11) and has dim =&gt; [batch_size, dec_hid_dim]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic">#encoder_outputs is the word representations from Eq. (6) </span>
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># and has dim =&gt; [src_len, batch_size, enc_hid_dim * 2]</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">forward</span>(self, hidden, encoder_outputs):
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Compute for q = W_q h_j^{e}</span>
</span></span><span style="display:flex;"><span>        query = self.W_q(hidden).unsqueeze(<span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Compute for k_t = W_k h_t^{d}</span>
</span></span><span style="display:flex;"><span>        key = self.W_k(encoder_outputs)
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Compute for score = qk^T / sqrt(d)</span>
</span></span><span style="display:flex;"><span>        score = torch.bmm(query, key.T(<span style="color:#099">1</span>, <span style="color:#099">2</span>)).squeeze(<span style="color:#099">1</span>) / np.sqrt(self.kq_dim)
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Compute alpha = softmax(score)</span>
</span></span><span style="display:flex;"><span>        alpha = F.softmax(score, dim=<span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#aaa;font-style:italic"># Compute a = sum(alpha_j v_j)</span>
</span></span><span style="display:flex;"><span>        attended_val = torch.sum(encoder_outputs * alpha.unsqueeze(<span style="color:#099">2</span>), dim=<span style="color:#099">1</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#00a">assert</span> attended_val.shape == (hidden.shape[<span style="color:#099">0</span>], encoder_outputs.shape[<span style="color:#099">2</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#00a">assert</span> alpha.shape == (hidden.shape[<span style="color:#099">0</span>], encoder_outputs.shape[<span style="color:#099">0</span>])
</span></span><span style="display:flex;"><span>        <span style="color:#00a">return</span> attended_val, alpha
</span></span></code></pre></div><br>
<h4 id="23-bleu-score-comparision"><strong>2.3. BLEU Score Comparision</strong><a hidden class="anchor" aria-hidden="true" href="#23-bleu-score-comparision">#</a></h4>
<p>Models were trained and evaluated using the <em>Dummy baseline</em>, <em>MeanPool</em>, and <em>attention mechanisms</em>. For each method, three independent runs were conducted, and the mean and variance of the results were calculated to compare performance and consistency.</p>
<div align="center">
<table>
  <thead>
      <tr>
          <th style="text-align: center">Attn.</th>
          <th style="text-align: center">PPL Mean</th>
          <th style="text-align: center">PPL Var</th>
          <th style="text-align: center">BLEU Mean</th>
          <th style="text-align: center">BLEU Var</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center">None</td>
          <td style="text-align: center">18.107</td>
          <td style="text-align: center">0.057</td>
          <td style="text-align: center">16.3</td>
          <td style="text-align: center">0.118</td>
      </tr>
      <tr>
          <td style="text-align: center">Mean</td>
          <td style="text-align: center">15.809</td>
          <td style="text-align: center">0.028</td>
          <td style="text-align: center">18.599</td>
          <td style="text-align: center">0.277</td>
      </tr>
      <tr>
          <td style="text-align: center">SDP</td>
          <td style="text-align: center">10.447</td>
          <td style="text-align: center">0.049</td>
          <td style="text-align: center">34.64</td>
          <td style="text-align: center">0.793</td>
      </tr>
  </tbody>
</table>
</div>
<p>The trends indicate that the scaled dot-product attention mechanism (SDP), significantly improve the model&rsquo;s performance, as lower perplexity and higher BLEU scores. However, the increased variance in BLEU scores with SDP suggests that there might be some variability caused by random initialization.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/nlp/">NLP</a></li>
      <li><a href="http://localhost:1313/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="http://localhost:1313/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="http://localhost:1313/tags/attention-mechanism/">Attention Mechanism</a></li>
      <li><a href="http://localhost:1313/tags/transformer/">Transformer</a></li>
      <li><a href="http://localhost:1313/tags/bleu-score/">BLEU Score</a></li>
      <li><a href="http://localhost:1313/tags/artificialintelligence/">ArtificialIntelligence</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Yong-Hwan Lee</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
