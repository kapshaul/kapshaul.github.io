<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>LLM Fine-Tunning for Code Vulnerability Detection | Yong-Hwan Lee</title>
<meta name="keywords" content="LLM, Quantization, Fine-Tunning, Code Vulnerability Detection">
<meta name="description" content="This study was carried out as a project at Oregon State University.">
<meta name="author" content="Yong-Hwan Lee,&thinsp;Shijie Zhao,&thinsp;James Flora,&thinsp;Yunhan Qiao">
<link rel="canonical" href="http://localhost:1313/projects/vul-detection/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d6cf4a8fa527330d9574f36d8d000fdaf90ca838ff09ab72fc27d3cb7ca1ddc5.css" integrity="sha256-1s9Kj6UnMw2VdPNtjQAP2vkMqDj/Caty/CfTy3yh3cU=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/projects/vul-detection/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript><meta property="og:title" content="LLM Fine-Tunning for Code Vulnerability Detection" />
<meta property="og:description" content="This study was carried out as a project at Oregon State University." />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://localhost:1313/projects/vul-detection/" />
<meta property="og:image" content="http://localhost:1313/image1.png" /><meta property="article:section" content="projects" />
<meta property="article:published_time" content="2024-06-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2024-08-20T00:00:00+00:00" />

<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="http://localhost:1313/image1.png" />
<meta name="twitter:title" content="LLM Fine-Tunning for Code Vulnerability Detection"/>
<meta name="twitter:description" content="This study was carried out as a project at Oregon State University."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Projects",
      "item": "http://localhost:1313/projects/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LLM Fine-Tunning for Code Vulnerability Detection",
      "item": "http://localhost:1313/projects/vul-detection/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LLM Fine-Tunning for Code Vulnerability Detection",
  "name": "LLM Fine-Tunning for Code Vulnerability Detection",
  "description": "This study was carried out as a project at Oregon State University.",
  "keywords": [
    "LLM", "Quantization", "Fine-Tunning", "Code Vulnerability Detection"
  ],
  "articleBody": " Download Paper Code Abstract This project replicates and builds upon the study by Shestov et al. (2024), aiming to validate and extend their findings. The original research focused on fine-tuning large language models (LLMs) for code vulnerability detection. The approach utilized LoRA (Low-Rank Adaptation), a technique that involves adding adapters within layers for fine-tuning. During this process, the original model parameters are frozen, and only the adapters are trained, making the training process more cost-effective.\nA key innovation of our work is the incorporation of our custom adaptation of QLoRA, which first quantizes the LLM to a 4-bit float, significantly reducing its size. For example, the 13B-WizardCoder model, originally around 26 GB and typically requiring more than 30 GB of VRAM, is reduced to approximately 7 GB after quantization. Following quantization, the LoRA technique is applied for fine-tuning.\nWhat is LoRA? Figure 1: LoRA adapter illustration\nFigure 1 illustrates how LoRA adapters can be significantly smaller than the original parameter sizes. The number of parameters for the $A$ adapter is $r \\times k$, and for the $B$ adapter, it is $d \\times r$. Considering the original parameter matrix is $d \\times k$, where both $d$ and $k$ are usually large for LLMs, choosing a small $r$ can effectively reduce the number of parameters. Thus, the original matrix $W \\in \\mathbb{R}^{d \\times k}$ is much larger than the combined size of the adapters $A \\in \\mathbb{R}^{r \\times k}$ and $B \\in \\mathbb{R}^{d \\times r}$.\nFor example, consider a layer in a LLM with a weight matrix $W \\in \\mathbb{R}^{1000 \\times 100}$. The number of parameters for $W$ is $1000 \\times 100 = 100,000$. If we set the LoRA rank to $r = 5$, the size of the LoRA adapters is only $1000 \\times 5 + 100 \\times 5 = 5,500$. This means the adapter size is around 5% of the original weight matrix $W$, which is significantly manageable for training as the original weight matrix $W$ remains frozen during the training phase.\nIn this project, we varied the dataset, sequence length, and the use of focal loss; measured the resulting performance changes compared to LoRA alone. The report for this project: PDF\nThis document provides detailed instructions for replicating our research project. It includes steps for setting up the necessary environment, making required code changes, running the model on a High-Performance Computing (HPC) cluster, and presenting the results.\nPreparation 1. Packages Installation (Python 3.10 used) pip install -r requirements.txt 2. Code Change For a debug model compatibility, Add the following function into the GPTBigCodeConfig class in the transformers package located at your_venv/lib/python3.10/site-packages/transformers/models/gpt_bigcode/configuration_gpt_bigcode.py: class GPTBigCodeConfig: # ... other methods and attributes ... def set_special_params(self, args): self.args = vars(args) Change the directory path at ./vul-llm-finetune/LLM/starcoder/run.py sys.path.append(\"my_path/vul-llm-finetune/LLM/starcoder\") Implementation Instruction 1. Request GPU from HPC (Based on OSU HPC server) srun -p dgxh –time=2-00:00:00 -c 2 –gres=gpu:2 –mem=20g –pty bash\nCluster: dgxh Time: 2-00:00:00 #CPUs: 2 #GPUs: 2 Memory: 20g 2. Use the below command to run (Specify the path for model saving and loading) Debug using a small model python vul-llm-finetune/LLM/starcoder/finetune/run.py \\ --dataset_tar_gz='vul-llm-finetune/Datasets/with_p3/java_k_1_strict_2023_06_30.tar.gz' \\ --split=\"train\" \\ --lora_r 8 \\ --seq_length 512 \\ --batch_size 1 \\ --gradient_accumulation_steps 160 \\ --learning_rate 1e-4 \\ --weight_decay 0.05 \\ --num_warmup_steps 2 \\ --log_freq=1 \\ --output_dir='vul-llm-finetune/outputs/results_test/' \\ --delete_whitespaces \\ --several_funcs_in_batch \\ --debug_on_small_model Train using LLM python vul-llm-finetune/LLM/starcoder/finetune/run.py \\ --dataset_tar_gz='vul-llm-finetune/Datasets/with_p3/java_k_1_strict_2023_06_30.tar.gz' \\ --load_quantized_model \\ --split=\"train\" \\ --lora_r 8 \\ --use_focal_loss \\ --focal_loss_gamma 1 \\ --seq_length 512 \\ --num_train_epochs 15 \\ --batch_size 1 \\ --gradient_accumulation_steps 160 \\ --learning_rate 1e-4 \\ --weight_decay 0.05 \\ --num_warmup_steps 2 \\ --log_freq=1 \\ --output_dir='vul-llm-finetune/outputs/results_0/' \\ --delete_whitespaces \\ --base_model starcoder \\ --several_funcs_in_batch Test python vul-llm-finetune/LLM/starcoder/finetune/run.py \\ --dataset_tar_gz='vul-llm-finetune/Datasets/with_p3/java_k_1_strict_2023_06_30.tar.gz' \\ --load_quantized_model \\ --split=\"test\" \\ --run_test_peft \\ --lora_r 8 \\ --seq_length 512 \\ --checkpoint_dir='vul-llm-finetune/outputs/results_0' \\ --model_checkpoint_path='final_checkpoint' \\ --delete_whitespaces \\ --base_model starcoder \\ --several_funcs_in_batch Result Dataset Sequence Length Large Function ROC AUC F1 Score GPU Training Time (hr) QLoRA X₁ without P₃ 512 ignore 0.53 0.65 Tesla T4 8.2 X₁ without P₃ 512 include 0.56 0.66 NVIDIA A100 x2 3.4 X₁ without P₃ 256 ignore 0.51 0.63 Tesla T4 2.9 X₁ with P₃ 512 ignore 0.68 0.14 RTX 4080 22.1 X₁ with P₃ 512 include 0.72 0.17 NVIDIA A100 x2 20.4 X₁ with P₃ 256 ignore 0.70 0.14 NVIDIA A100 x2 18.3 LoRA X₁ without P₃ 2048 include 0.69 0.71 NVIDIA V100 x8 X₁ with P₃ 2048 include 0.86 0.27 NVIDIA V100 x8 Conclusion In this paper, we recreate the findings of Shestov et al. in which we finetune the LLM, WizardCoder, for code vulnerability detection. Whilst the original authors use LoRA to do so, we employ QLoRA to cut down on overall model size and are able to train such a model on a consumer-grade GPU. Despite this, we see significant degradation in performance metrics though it is clear that the model is still doing some sort of learning. Further, we perform experimentation on the hyperparameters sequence length and include large function. We are able to conclude that including large functions is a strict positive for the model’s learning capabilities, but the evidence on sequence length is inconclusive due to a baffling experiment with much higher results than the rest.\nReference [1] Shestov, A., Levichev, R., Mussabayev, R., Maslov, E., Cheshkov, A., \u0026 Zadorozhny, P. (2024). Finetuning Large Language Models for Vulnerability Detection. arXiv preprint arXiv:2401.17010. Retrieved from https://arxiv.org/abs/2401.17010.\n[2] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., \u0026 Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685. Retrieved from https://arxiv.org/abs/2106.09685.\n[3] Dettmers, T., Pagnoni, A., Holtzman, A., \u0026 Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314. Retrieved from https://arxiv.org/abs/2305.14314.\n",
  "wordCount" : "931",
  "inLanguage": "en",
  "image":"http://localhost:1313/image1.png","datePublished": "2024-06-13T00:00:00Z",
  "dateModified": "2024-08-20T00:00:00Z",
  "author":[{
    "@type": "Person",
    "name": "Yong-Hwan Lee"
  }, {
    "@type": "Person",
    "name": "Shijie Zhao"
  }, {
    "@type": "Person",
    "name": "James Flora"
  }, {
    "@type": "Person",
    "name": "Yunhan Qiao"
  }],
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/projects/vul-detection/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Yong-Hwan Lee",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"
  onload="renderMathInElement(document.body);"></script>

<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false},
            {left: "\\begin{equation}", right: "\\end{equation}", display: true},
            {left: "\\begin{equation*}", right: "\\end{equation*}", display: true},
            {left: "\\begin{align}", right: "\\end{align}", display: true},
            {left: "\\begin{align*}", right: "\\end{align*}", display: true},
            {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
            {left: "\\begin{gather}", right: "\\end{gather}", display: true},
            {left: "\\begin{CD}", right: "\\end{CD}", display: true},
          ],
          throwOnError : false
        });
    });
</script>
 


</head>

<body class="" id="top">

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Yong-Hwan Lee">
                <img src="http://localhost:1313/favicon.ico" alt="" aria-label="logo"
                    height="18"
                    width="18">Yong-Hwan Lee</a>
            <div class="logo-switches">
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/studies/" title="Studies">
                    <span>Studies</span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      LLM Fine-Tunning for Code Vulnerability Detection
    </h1>
    <div class="post-meta"><span title='2024-06-13 00:00:00 +0000 UTC'>June 2024</span>&nbsp;&middot;&nbsp;Yong-Hwan Lee,&thinsp;Shijie Zhao,&thinsp;James Flora,&thinsp;Yunhan Qiao

</div>
  </header> 
  <div class="post-content"><hr>
<h5 id="download">Download</h5>
<ul>
<li><a href="paper.pdf">Paper</a></li>
<li><a href="https://github.com/kapshaul/LLM-finetune-vuln-detection" target="_blank">Code</a></li>
</ul>
<hr>
<h5 id="abstract">Abstract</h5>
<p>This project replicates and builds upon the <a href="https://arxiv.org/pdf/2401.17010" target="_blank">study</a> by <em>Shestov et al. (2024)</em>, aiming to validate and extend their findings. The original research focused on fine-tuning large language models (LLMs) for code vulnerability detection. The approach utilized <code>LoRA</code> (Low-Rank Adaptation), a technique that involves adding adapters within layers for fine-tuning. During this process, the original model parameters are <em>frozen</em>, and only the adapters are trained, making the training process more cost-effective.</p>
<p>A key innovation of our work is the incorporation of our custom adaptation of <code>QLoRA</code>, which first quantizes the LLM to a <em>4-bit float</em>, significantly reducing its size. For example, the <strong>13B-WizardCoder model</strong>, originally around <em>26 GB</em> and typically requiring more than <em>30 GB</em> of VRAM, is reduced to approximately <em>7 GB</em> after quantization. Following quantization, the <code>LoRA</code> technique is applied for fine-tuning.</p>
<hr>
<h5 id="what-is-lora">What is LoRA?</h5>
<div align="center">
<img src="lora.png" width="500">
<p><strong>Figure 1</strong>: LoRA adapter illustration</p>
</div>
<p>Figure 1 illustrates how LoRA adapters can be significantly smaller than the original parameter sizes. The number of parameters for the $A$ adapter is $r \times k$, and for the $B$ adapter, it is $d \times r$. Considering the original parameter matrix is $d \times k$, where both $d$ and $k$ are usually large for LLMs, choosing a small $r$ can effectively reduce the number of parameters. Thus, the original matrix $W \in \mathbb{R}^{d \times k}$ is much larger than the combined size of the adapters $A \in \mathbb{R}^{r \times k}$ and $B \in \mathbb{R}^{d \times r}$.</p>
<blockquote>
<p>For example, consider a layer in a LLM with a weight matrix $W \in \mathbb{R}^{1000 \times 100}$. The number of parameters for $W$ is $1000 \times 100 = 100,000$. If we set the LoRA rank to $r = 5$, the size of the LoRA adapters is only $1000 \times 5 + 100 \times 5 = 5,500$. This means the adapter size is around 5% of the original weight matrix $W$, which is significantly manageable for training as the original weight matrix $W$ remains frozen during the training phase.</p></blockquote>
<br>
<p>In this project, we varied the <code>dataset</code>, <code>sequence length</code>, and <code>the use of focal loss</code>; measured the resulting performance changes compared to LoRA alone.
The report for this project: <a href="paper.pdf">PDF</a></p>
<p>This document provides detailed instructions for replicating our research project. It includes steps for setting up the necessary environment, making required code changes, running the model on a High-Performance Computing (HPC) cluster, and presenting the results.</p>
<hr>
<h5 id="preparation">Preparation</h5>
<h5 id="1-packages-installation-python-310-used"><strong>1. Packages Installation (Python 3.10 used)</strong></h5>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>pip install -r requirements.txt
</span></span></code></pre></div><h5 id="2-code-change"><strong>2. Code Change</strong></h5>
<ul>
<li>For a debug model compatibility, Add the following function into the <code>GPTBigCodeConfig</code> class in the transformers package located at <code>your_venv/lib/python3.10/site-packages/transformers/models/gpt_bigcode/configuration_gpt_bigcode.py</code>:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00a">class</span> <span style="color:#0a0;text-decoration:underline">GPTBigCodeConfig</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#aaa;font-style:italic"># ... other methods and attributes ...</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00a">def</span> <span style="color:#0a0">set_special_params</span>(self, args):
</span></span><span style="display:flex;"><span>        self.args = <span style="color:#0aa">vars</span>(args)
</span></span></code></pre></div><ul>
<li>Change the directory path at <code>./vul-llm-finetune/LLM/starcoder/run.py</code></li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sys.path.append(<span style="color:#a50">&#34;my_path/vul-llm-finetune/LLM/starcoder&#34;</span>)
</span></span></code></pre></div><hr>
<h5 id="implementation-instruction">Implementation Instruction</h5>
<h5 id="1-request-gpu-from-hpc-based-on-osu-hpc-server"><strong>1. Request GPU from HPC (Based on OSU HPC server)</strong></h5>
<p>srun -p dgxh &ndash;time=2-00:00:00 -c 2 &ndash;gres=gpu:2 &ndash;mem=20g &ndash;pty bash</p>
<ul>
<li>Cluster: dgxh</li>
<li>Time: 2-00:00:00</li>
<li>#CPUs: 2</li>
<li>#GPUs: 2</li>
<li>Memory: 20g</li>
</ul>
<h5 id="2-use-the-below-command-to-run-specify-the-path-for-model-saving-and-loading"><strong>2. Use the below command to run (Specify the path for model saving and loading)</strong></h5>
<ul>
<li>Debug using a small model</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python vul-llm-finetune/LLM/starcoder/finetune/run.py <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--dataset_tar_gz=<span style="color:#a50">&#39;vul-llm-finetune/Datasets/with_p3/java_k_1_strict_2023_06_30.tar.gz&#39;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--split=<span style="color:#a50">&#34;train&#34;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--lora_r <span style="color:#099">8</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--seq_length <span style="color:#099">512</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--batch_size <span style="color:#099">1</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--gradient_accumulation_steps <span style="color:#099">160</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--learning_rate 1e-4 <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--weight_decay 0.05 <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--num_warmup_steps <span style="color:#099">2</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--log_freq=<span style="color:#099">1</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--output_dir=<span style="color:#a50">&#39;vul-llm-finetune/outputs/results_test/&#39;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--delete_whitespaces <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--several_funcs_in_batch <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--debug_on_small_model
</span></span></code></pre></div><ul>
<li>Train using LLM</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python vul-llm-finetune/LLM/starcoder/finetune/run.py <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--dataset_tar_gz=<span style="color:#a50">&#39;vul-llm-finetune/Datasets/with_p3/java_k_1_strict_2023_06_30.tar.gz&#39;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--load_quantized_model <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--split=<span style="color:#a50">&#34;train&#34;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--lora_r <span style="color:#099">8</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--use_focal_loss <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--focal_loss_gamma <span style="color:#099">1</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--seq_length <span style="color:#099">512</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--num_train_epochs <span style="color:#099">15</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--batch_size <span style="color:#099">1</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--gradient_accumulation_steps <span style="color:#099">160</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--learning_rate 1e-4 <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--weight_decay 0.05 <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--num_warmup_steps <span style="color:#099">2</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--log_freq=<span style="color:#099">1</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--output_dir=<span style="color:#a50">&#39;vul-llm-finetune/outputs/results_0/&#39;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--delete_whitespaces <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--base_model starcoder <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--several_funcs_in_batch
</span></span></code></pre></div><ul>
<li>Test</li>
</ul>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>python vul-llm-finetune/LLM/starcoder/finetune/run.py <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--dataset_tar_gz=<span style="color:#a50">&#39;vul-llm-finetune/Datasets/with_p3/java_k_1_strict_2023_06_30.tar.gz&#39;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--load_quantized_model <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--split=<span style="color:#a50">&#34;test&#34;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--run_test_peft <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--lora_r <span style="color:#099">8</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--seq_length <span style="color:#099">512</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--checkpoint_dir=<span style="color:#a50">&#39;vul-llm-finetune/outputs/results_0&#39;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--model_checkpoint_path=<span style="color:#a50">&#39;final_checkpoint&#39;</span> <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--delete_whitespaces <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--base_model starcoder <span style="color:#a50">\
</span></span></span><span style="display:flex;"><span><span style="color:#a50"></span>--several_funcs_in_batch
</span></span></code></pre></div><hr>
<h5 id="result">Result</h5>
<table>
  <thead>
      <tr>
          <th style="text-align: center"></th>
          <th style="text-align: center">Dataset</th>
          <th style="text-align: center">Sequence Length</th>
          <th style="text-align: center">Large Function</th>
          <th style="text-align: center">ROC AUC</th>
          <th style="text-align: center">F1 Score</th>
          <th style="text-align: center">GPU</th>
          <th style="text-align: center">Training Time (hr)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><strong>QLoRA</strong></td>
          <td style="text-align: center">X₁ without P₃</td>
          <td style="text-align: center">512</td>
          <td style="text-align: center">ignore</td>
          <td style="text-align: center">0.53</td>
          <td style="text-align: center">0.65</td>
          <td style="text-align: center">Tesla T4</td>
          <td style="text-align: center">8.2</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">X₁ without P₃</td>
          <td style="text-align: center">512</td>
          <td style="text-align: center">include</td>
          <td style="text-align: center">0.56</td>
          <td style="text-align: center">0.66</td>
          <td style="text-align: center">NVIDIA A100 x2</td>
          <td style="text-align: center">3.4</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">X₁ without P₃</td>
          <td style="text-align: center">256</td>
          <td style="text-align: center">ignore</td>
          <td style="text-align: center">0.51</td>
          <td style="text-align: center">0.63</td>
          <td style="text-align: center">Tesla T4</td>
          <td style="text-align: center">2.9</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">X₁ with P₃</td>
          <td style="text-align: center">512</td>
          <td style="text-align: center">ignore</td>
          <td style="text-align: center">0.68</td>
          <td style="text-align: center">0.14</td>
          <td style="text-align: center">RTX 4080</td>
          <td style="text-align: center">22.1</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">X₁ with P₃</td>
          <td style="text-align: center">512</td>
          <td style="text-align: center">include</td>
          <td style="text-align: center">0.72</td>
          <td style="text-align: center">0.17</td>
          <td style="text-align: center">NVIDIA A100 x2</td>
          <td style="text-align: center">20.4</td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">X₁ with P₃</td>
          <td style="text-align: center">256</td>
          <td style="text-align: center">ignore</td>
          <td style="text-align: center">0.70</td>
          <td style="text-align: center">0.14</td>
          <td style="text-align: center">NVIDIA A100 x2</td>
          <td style="text-align: center">18.3</td>
      </tr>
      <tr>
          <td style="text-align: center"><strong>LoRA</strong></td>
          <td style="text-align: center">X₁ without P₃</td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">include</td>
          <td style="text-align: center">0.69</td>
          <td style="text-align: center">0.71</td>
          <td style="text-align: center">NVIDIA V100 x8</td>
          <td style="text-align: center"></td>
      </tr>
      <tr>
          <td style="text-align: center"></td>
          <td style="text-align: center">X₁ with P₃</td>
          <td style="text-align: center">2048</td>
          <td style="text-align: center">include</td>
          <td style="text-align: center">0.86</td>
          <td style="text-align: center">0.27</td>
          <td style="text-align: center">NVIDIA V100 x8</td>
          <td style="text-align: center"></td>
      </tr>
  </tbody>
</table>
<hr>
<h5 id="conclusion">Conclusion</h5>
<p>In this paper, we recreate the findings of <em>Shestov et al</em>. in which we finetune the LLM, WizardCoder, for code vulnerability detection. Whilst the original authors use LoRA  to do so, we employ QLoRA to cut down on overall model size and are able to train such a model on a consumer-grade GPU. Despite this, we see significant degradation in performance metrics though it is clear that the model is still doing some sort of <em>learning</em>. Further, we perform experimentation on the hyperparameters <em>sequence length</em> and <em>include large function</em>. We are able to conclude that including large functions is a strict positive for the model’s learning capabilities, but the evidence on sequence length is inconclusive due to a baffling experiment with much higher results than the rest.</p>
<hr>
<h5 id="reference">Reference</h5>
<p>[1] Shestov, A., Levichev, R., Mussabayev, R., Maslov, E., Cheshkov, A., &amp; Zadorozhny, P. (2024). <em>Finetuning Large Language Models for Vulnerability Detection</em>. arXiv preprint arXiv:2401.17010. Retrieved from <a href="https://arxiv.org/abs/2401.17010" target="_blank">https://arxiv.org/abs/2401.17010</a>.</p>
<p>[2] Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., &amp; Chen, W. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685. Retrieved from <a href="https://arxiv.org/abs/2106.09685" target="_blank">https://arxiv.org/abs/2106.09685</a>.</p>
<p>[3] Dettmers, T., Pagnoni, A., Holtzman, A., &amp; Zettlemoyer, L. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. arXiv preprint arXiv:2305.14314. Retrieved from <a href="https://arxiv.org/abs/2305.14314" target="_blank">https://arxiv.org/abs/2305.14314</a>.</p>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/llm/">LLM</a></li>
      <li><a href="http://localhost:1313/tags/quantization/">Quantization</a></li>
      <li><a href="http://localhost:1313/tags/fine-tunning/">Fine-Tunning</a></li>
      <li><a href="http://localhost:1313/tags/code-vulnerability-detection/">Code Vulnerability Detection</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2025 <a href="http://localhost:1313/">Yong-Hwan Lee</a></span> ·     
    <span>
    Powered by 
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/pmichaillat/hugo-website/" rel="noopener" target="_blank">a modified version</a>
         of 
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>
</html>
